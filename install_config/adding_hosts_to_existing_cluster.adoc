[[install-config-adding-hosts-to-cluster]]
= Adding Hosts to an Existing Cluster
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

[[adding-nodes-advanced]]
== Adding Hosts

You can add new hosts to your cluster by running the *_scaleup.yml_* playbook.
This playbook queries the master, generates and distributes new certificates for
the new hosts, then runs the configuration playbooks on the new hosts only.
Before running the *_scaleup.yml_* playbook, complete all prerequisite
xref:../install_config/install/host_preparation.adoc#install-config-install-host-preparation[host
preparation] steps.

[IMPORTANT]
====
The scaleup playbook only configures the new host. It does not update *_NO_PROXY_* in master services and it does not restart master services.
====

You must have an existing inventory file (for example, *_/etc/ansible/hosts_*)
that is representative of your current cluster configuration in order to run the
*_scaleup.yml_* playbook.
ifdef::openshift-enterprise[]
If you previously used the `atomic-openshift-installer` command to run your
installation, you can check *_~/.config/openshift/hosts_* (previously located at
*_~/.config/openshift/.ansible/hosts_*) for the last inventory file that the
installer generated, and use or modify that as needed as your inventory file.
You must then specify the file location with `-i` when calling
`ansible-playbook` later.
endif::[]

[IMPORTANT]
====
See the
xref:../scaling_performance/cluster_limits.adoc#scaling-performance-cluster-limits[cluster
limits] section for recommended maximum number of nodes.
====

To add a host to an existing cluster:

. Ensure you have the latest playbooks by updating the *atomic-openshift-utils*
package:
+
----
# yum update atomic-openshift-utils
----

. Edit your *_/etc/ansible/hosts_* file and add *new_<host_type>* to the
*[OSEv3:children]* section:
+
For example, to add a new node host, add *new_nodes*:
+
----
[OSEv3:children]
masters
nodes
new_nodes
----
+
To add new master hosts, add *new_masters*.

. Create a *[new_<host_type>]* section much like an existing section,
specifying host information for any new hosts you want to add. For example,
when adding a new node:
+
----
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"

[new_nodes]
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
+
See
xref:../install_config/install/advanced_install.adoc#advanced-host-variables[Configuring
Host Variables] for more options.
+
When adding new masters, hosts added to the *[new_masters]* section must also be
added to the *[new_nodes]* section. This ensures the new master host is part of
the OpenShift SDN.
+
----
[masters]
master[1:2].example.com

[new_masters]
master3.example.com

[nodes]
master[1:2].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"

[new_nodes]
master3.example.com
----
+
Masters are also automatically marked as unschedulable for pod placement by the
installer.
+
[IMPORTANT]
====
If you label a master host with the `region=infra` label and have no other
dedicated infrastructure nodes, you must also explicitly mark the host as
schedulable by adding `openshift_schedulable=true` to the entry. Otherwise, the
registry and router pods cannot be placed anywhere.
====

. Run the *_scaleup.yml_* playbook. If your inventory file is located somewhere
other than the default of *_/etc/ansible/hosts_*, specify the location with the
`-i option`.
+
For additional nodes:
+
----
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml
----
+
For additional masters:
+
----
# ansible-playbook [-i /path/to/file] \
    /usr/share/ansible/openshift-ansible/playbooks/openshift-master/scaleup.yml
----

. After the playbook completes successfully,
xref:../install_config/install/advanced_install.adoc#advanced-verifying-the-installation[verify the installation].

. Finally, move any hosts you had defined in the *[new_<host_type>]* section
into their appropriate section (but leave the *[new_<host_type>]* section
definition itself in place) so that subsequent runs using this inventory file
are aware of the nodes but do not handle them as new nodes. For example, when
adding new nodes:
+
----
[nodes]
master[1:3].example.com
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"

[new_nodes]
----

[[adding-etcd-hosts-to-existing-cluster]]
== Adding etcd Hosts to existing Cluster
You can add new etcd hosts to your cluster by running the _etcd scaleup_
playbook. This playbook queries the master, generates and distributes new
certificates for the new hosts, and then runs the configuration playbooks on the
new hosts only. Before running the etcd  *_scaleup.yml_* playbook, complete all
prerequisite
xref:../install_config/install/host_preparation.adoc#install-config-install-host-preparation[host
preparation] steps.

To add an etcd host to an existing cluster:

. Ensure you have the latest playbooks by updating the *atomic-openshift-utils* package:
+
[source, bash]
----
$ yum update atomic-openshift-utils
----

. Edit your *_/etc/ansible/hosts_* file, add *new_<host_type>* to the
*[OSEv3:children]* group and add hosts under the *new_<host_type>* group:
+
For example, to add a new etcd, add *new_etcd*:
+
----
[OSEv3:children]
masters
nodes
etcd
new_etcd

[etcd]
etcd1.example.com
etcd2.example.com

[new_etcd]
etcd3.example.com
----

. Run the etcd *_scaleup.yml_* playbook. If your inventory file is located somewhere other than the default of *_/etc/ansible/hosts_*, specify the location with the `-i` option.
+
[source, bash]
----
$ ansible-playbook [-i /path/to/file] \
  /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/scaleup.yml
----

. After the playbook completes successfully,
xref:../install_config/install/advanced_install.adoc#advanced-verifying-the-installation[verify the installation].

[[replacing-existing-masters]]
== Replacing Existing Masters with etcd co-located

Follow these steps when you are migrating your machines to a different data
center and the network and IPs assigned to it will change.

. Back up the primary xref:../admin_guide/backup_restore.adoc#etcd-backup[etcd]
and xref:../admin_guide/backup_restore.adoc#master-backup[master] nodes.
+
[IMPORTANT]
====
Ensure that you back up the *_/etc/etcd/ca/_* directory, as noted in the
xref:../admin_guide/backup_restore.adoc#backup-restore-adding-etcd-hosts[Backup
and Restore] topic.
====

. Provision as many new machines as there are masters to replace.

. Add or expand the cluster. for example, if you want to add 3 masters with etcd
co-located, scale up 3 master nodes or 3 etcd nodes.

.. Add a xref:adding-nodes-advanced[master]. In step 3 of that process, add the
host of the new data center in `[new_masters]` and `[new_nodes]` and run the
xref:adding-nodes-advanced[master *_scaleup.yml_* playbook].

.. Put the same host in the xref:adding-etcd-hosts-to-existing-cluster[etcd]
section and run the etcd *_scaleup.yml_* playbook.

.. Verify that the host was added:
+
----
# oc get nodes
----

.. Verify that the master host IP was added:
+
----
# oc get ep kubernetes
----

.. Verify that etcd was added. The value of `ETCDCTL_API` depends on the version
being used:
+
----
# source /etc/etcd/etcd.conf
# ETCDCTL_API=2 etcdctl --cert-file=$ETCD_PEER_CERT_FILE --key-file=$ETCD_PEER_KEY_FILE \
  --ca-file=/etc/etcd/ca.crt --endpoints=$ETCD_LISTEN_CLIENT_URLS member list
----

.. Copy  *_/etc/origin/master/ca.serial.txt_* from the *_/etc/origin/master_*
directory to the new master host that is listed first in your inventory file. By
default, this is *_/etc/ansible/hosts_*.

. Remove the etcd hosts.

.. Copy the *_/etc/etcd/ca_* directory to the new etcd host that is listed first in
your inventory file. By default, this is *_/etc/ansible/hosts_*.

.. Remove the old etcd clients from the *_master-config.yaml_* file:
+
----
# grep etcdClientInfo -A 11 /etc/origin/master/master-config.yaml
----

.. Restart the masters:
+
----
# systemctl restart atomic-openshift-master-*
----

.. Remove the old etcd members from the cluster. The value of `ETCDCTL_API` depends
on the version being used:
+
----
# source /etc/etcd/etcd.conf
# ETCDCTL_API=2 etcdctl --cert-file=$ETCD_PEER_CERT_FILE --key-file=$ETCD_PEER_KEY_FILE \
  --ca-file=/etc/etcd/ca.crt --endpoints=$ETCD_LISTEN_CLIENT_URLS member list
----

.. Take the IDs from the output of the command above and remove the old members
using the IDs:
+
----
# etcdctl --cert-file=$ETCD_PEER_CERT_FILE --key-file=$ETCD_PEER_KEY_FILE \
  --ca-file=/etc/etcd/ca.crt --endpoints=$ETCD_LISTEN_CLIENT_URL member remove 1609b5a3a078c227
----

.. Stop and disable the etcd services on the old etcd hosts:
+
----
# systemctl stop etcd
# systemctl disable etcd
----

. Shut down old master API and controller services:
+
----
# systemctl stop atomic-openshift-master-api
----

. Remove the master nodes from the HA proxy configuration, which was installed as
a load balancer by default during the native installation process.

. Decommission the machine.

.. Stop the `atomic-openshift-node` service on the
master to be removed:
+
----
# systemctl stop atomic-openshift-node
----

.. Delete the node resource:
+
----
# oc delete node
----

[[migrating-the-nodes]]
== Migrating the Nodes

You can migrate nodes individually or in groups (of 2, 5, 10, and so on),
depending on what you are comfortable with and how the services on the node are
run and scaled.

. For the migration node or nodes, provision new VMs for the node's use in the new
data center.

. To add the new node, xref:adding-nodes-advanced[scale up the infrastructure].
Ensure the labels for the new node are set properly and that your new API
servers are added to your load balancer and successfully serving traffic.

. Evaluate and scale down.

.. Mark the current node (in the old data center)
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unscheduled].

.. xref:../admin_guide/manage_nodes.adoc#evacuating-pods-on-nodes[Evacuate the
node], so that pods on it are scheduled to other nodes.

.. Verify that the evacuated services are running on the new nodes.

. Remove the node.

.. Verify that the node is empty and does not have running processes.

.. Stop the service or delete the node.
