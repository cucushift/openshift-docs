////
etcd backup

Module included in the following assemblies:

* day_two_guide/host_level_tasks.adoc
* day_two_guide/environment_backup.adoc
////

etcd is the key value store for all object definitions, as well as the
persistent master state. Other components watch for changes, then bring
themselves into the desired state.

{product-title} versions prior to 3.5 used etcd version 2 (v2), while 3.5 and
later use version 3 (v3). The data model between the two versions of etcd is
different. etcd v3 can use both the v2 and v3 data model, whereas etcd v2 can
only use the v2 data model. In an etcd v3 server, the v2 and v3 data stores
exist in parallel and are independent.

For both v2 and v3 operations, you can use the `ETCDCTL_API` environment
variable to use the proper API:

----
$ etcdctl -v
etcdctl version: 3.2.5
API version: 2
$ ETCDCTL_API=3 etcdctl version
etcdctl version: 3.2.5
API version: 3.2
----

See
link:https://docs.openshift.com/container-platform/3.7/upgrading/migrating_etcd.html[Migrating etcd Data (v2 to v3) section] in the {product-title} 3.7 documentation for
information about how to migrate to v3.

The etcd backup process is composed of two different procedures:

* Configuration backup: Including the required etcd configuration and
certificates
* Data backup: Including both v2 and v3 data model.

The data backup procedure can be done on any host that has connectivity to the
etcd cluster, where the proper certificates are provided, and where the
`etcdctl` tool is installed.

[NOTE]
====
The backup files must be copied to an external system, ideally outside the
{product-title} environment, and then encrypted.
====

The etcd configuration files to be preserved are all stored in the `/etc/etcd`
directory of the instances where etcd is running. This includes the etcd
configuration file (`/etc/etcd/etcd.conf`) and the required certificates for
cluster communication. All those files are generated at installation time by the
Ansible installer.

To back up the etcd configuration:

----
$ ssh master-0
# mkdir -p /backup/etcd-config-$(date +%Y%m%d)/
# cp -R /etc/etcd/ /backup/etcd-config-$(date +%Y%m%d)/
----

[NOTE]
====
The backup is to be performed on every etcd member of the cluster
as the certificates and configuration files are unique.
====

[id='etcd-data-backup_{context}']
==== etcd data backup

[discrete]
== Prerequisites

[NOTE]
====
The {product-title} installer creates aliases to avoid typing all the
flags named `etcdctl2` for etcd v2 tasks and `etcdctl3` for etcd v3 tasks.

However, the `etcdctl3` alias does not provide the full endpoint list to the
`etcdctl` command, so the `--endpoints` option with all the endpoints must be
provided.
====

Before backing up etcd:

* `etcdctl` binaries should be available or, in containerized installations, the `rhel7/etcd` container should be available
* Ensure connectivity with the etcd cluster (port 2379/tcp)
* Ensure the proper certificates to connect to the etcd cluster

. To ensure the etcd cluster is working, check its health:
+
----
# etcdctl --cert-file=/etc/etcd/peer.crt \
          --key-file=/etc/etcd/peer.key \
          --ca-file=/etc/etcd/ca.crt \
          --peers="https://*master-0.example.com*:2379,\
          https://*master-1.example.com*:2379,\
          https://*master-2.example.com*:2379"\
          cluster-health
member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
cluster is healthy
----
+
Or if using the etcd v3 API:
+
----
# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
          --key=/etc/etcd/peer.key \
          --cacert="/etc/etcd/ca.crt" \
          --endpoints="https://*master-0.example.com*:2379,\
            https://*master-1.example.com*:2379,\
            https://*master-2.example.com*:2379"
            endpoint health
https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
----

. Check the member list:
+
----
# etcdctl2 member list
2a371dd20f21ca8d: name=master-1.example.com peerURLs=https://192.168.55.12:2380 clientURLs=https://192.168.55.12:2379 isLeader=false
40bef1f6c79b3163: name=master-0.example.com peerURLs=https://192.168.55.8:2380 clientURLs=https://192.168.55.8:2379 isLeader=false
95dc17ffcce8ee29: name=master-2.example.com peerURLs=https://192.168.55.13:2380 clientURLs=https://192.168.55.13:2379 isLeader=true
----
+
Or, if using etcd the v3 API:
+
----
# etcdctl3 member list
2a371dd20f21ca8d, started, master-1.example.com, https://192.168.55.12:2380, https://192.168.55.12:2379
40bef1f6c79b3163, started, master-0.example.com, https://192.168.55.8:2380, https://192.168.55.8:2379
95dc17ffcce8ee29, started, master-2.example.com, https://192.168.55.13:2380, https://192.168.55.13:2379
----

[discrete]
== Procedure

[NOTE]
====
While the `etcdctl backup` command is used to perform the backup, etcd v3 has
no concept of a "backup". Instead, a "snapshot" may either be taken from a live
member with the `etcdctl snapshot save` command or by copying the
`member/snap/db` file from an etcd data directory.

The `etcdctl backup` command rewrites some of the metadata contained in the
backup (specifically, the node ID and cluster ID), which means that in the
backup, the node loses its former identity. In order to recreate a cluster from
the backup, a new, single-node cluster is created, then the rest of the nodes
join the cluster. The metadata is rewritten to prevent the new node from
inadvertently being joined onto an existing cluster.
====

. If you use the v2 api and  *etcd* is running on more than one host, stop it 
on each host:
+
----
# sudo systemctl stop etcd
----
+
Although this step is not strictly necessary, doing so ensures that the *etcd*
data is fully synchronized.

. Perform the backup:
+
----
# mkdir -p /backup/etcd-$(date +%Y%m%d)
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
# cp /var/lib/etcd/member/snap/db /backup/etcd-$(date +%Y%m%d)
----
+
The `etcdctl2 backup` command creates etcd v2 data backup where copying the `db`
file while the etcd service is not running is equivalent to running `etcdctl3
snapshot` for etcd v3 data backup:
+
----
# mkdir -p /backup/etcd-$(date +%Y%m%d)
# etcdctl3 snapshot save */backup/etcd-$(date +%Y%m%d)*/db
Snapshot saved at /backup/etcd-<date>/db
# systemctl stop etcd.service
# etcdctl2 backup \
    --data-dir /var/lib/etcd \
    --backup-dir /backup/etcd-$(date +%Y%m%d)
# systemctl start etcd.service
----
+
[NOTE]
====
The `etcdctl snapshot save` command requires the etcd service to be running.
====
+
In this example, a `/backup/etcd-<date>/` directory is created, where `<date>`
represents the current date, which must be an external NFS share, S3 bucket, or
any external storage location.
+
In the case of an all-in-one cluster, the etcd data directory is located in
`/var/lib/origin/openshift.local.etcd`
